# -*- coding: utf-8 -*-
"""IIIT_assignment2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WFiY8wAOC0Xl7mck1tSytk1xdb9166aT
"""

!pip install torch torchvision torchaudio --upgrade

from google.colab import drive

drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/Assignment2/Assignment2/dataset/Pride_and_Prejudice-Jane_Austen.txt'
with open(dataset_path, 'r', encoding='utf-8') as f:
    text = f.read()
print(text[:500])  # print first 500 characters as a check

tokens = text.split()

unique_tokens = sorted(set(tokens))
vocab = {token: idx for idx, token in enumerate(unique_tokens)}

encoded_text = [vocab[token] for token in tokens]

print(f"Number of tokens in dataset: {len(tokens)}")
print(f"Vocabulary size: {len(vocab)}")
print(f"First 10 tokens: {tokens[:10]}")
print(f"First 10 encoded tokens: {encoded_text[:10]}")

import torch
from torch.utils.data import Dataset, DataLoader

sequence_length = 5
batch_size = 32

class TextDataset(Dataset):
    def __init__(self, encoded_text, seq_len):
        self.encoded_text = encoded_text
        self.seq_len = seq_len

    def __len__(self):
        return len(self.encoded_text) - self.seq_len

    def __getitem__(self, idx):
        # Input sequence
        x = torch.tensor(self.encoded_text[idx:idx+self.seq_len], dtype=torch.long)
        # Target sequence (next tokens)
        y = torch.tensor(self.encoded_text[idx+1:idx+1+self.seq_len], dtype=torch.long)
        return x, y

dataset = TextDataset(encoded_text, sequence_length)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

for x_batch, y_batch in dataloader:
    print("Input batch shape:", x_batch.shape)   # (batch_size, sequence_length)
    print("Target batch shape:", y_batch.shape)  # (batch_size, sequence_length)
    break

import torch.nn as nn

class LSTMLanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):
        super(LSTMLanguageModel, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        # x: batch of token indices, shape (batch_size, seq_length)
        embed = self.embedding(x)  # (batch_size, seq_length, embed_size)

        if hidden is None:
            output, hidden = self.lstm(embed)
        else:
            output, hidden = self.lstm(embed, hidden)

        # output shape: (batch_size, seq_length, hidden_size)
        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)

        return logits, hidden

import torch.optim as optim

embed_size = 128
hidden_size = 256
num_layers = 2
num_epochs = 10
learning_rate = 0.002

model = LSTMLanguageModel(len(vocab), embed_size, hidden_size, num_layers)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

def train_with_loss_plot(model, dataloader, criterion, optimizer, device, num_epochs):
    model.train()
    epoch_losses = [] # To store average loss per epoch
    for epoch in range(num_epochs):
        total_loss = 0
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs, _ = model(inputs)

            # Reshape outputs and targets for calculating loss
            loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        epoch_losses.append(avg_loss)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
    return epoch_losses

train_with_loss_plot(model, dataloader, criterion, optimizer, device, num_epochs)

torch.save(model.state_dict(), 'trained_language_model.pth')

import math

def evaluate_prediction_difficulty(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs, _ = model(inputs)
            loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))
            total_loss += loss.item()
    avg_loss = total_loss / len(dataloader)
    prediction_difficulty = math.exp(avg_loss)
    return prediction_difficulty

prediction_difficulty = evaluate_prediction_difficulty(model, dataloader, criterion, device)
print(f'Validation Prediction Difficulty: {prediction_difficulty:.2f}')

def run_experiments(vocab_size, dataloader, val_dataloader, criterion, device, experiment_configs, num_epochs=5):
    results = []
    for i, config in enumerate(experiment_configs):
        print(f"\n=== Experiment {i+1}: {config} ===")

        model = LSTMLanguageModel(
            vocab_size=vocab_size,
            embed_size=config['embed_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers']
        ).to(device)

        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])

        # Train the model
        train_with_loss_plot(model, dataloader, criterion, optimizer, device, num_epochs)

        # Evaluate on validation set
        val_score = evaluate_prediction_difficulty(model, val_dataloader, criterion, device)  # change name if you renamed perplexity

        print(f"Validation Score (Prediction Difficulty): {val_score:.2f}")

        results.append({
            'config': config,
            'validation_score': val_score,
            'model': model
        })

    return results

# Define experiment configurations for different fitting cases
experiment_configs = [
    # Underfitting: Small model, fewer epochs
    {'embed_size': 64, 'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01},
    # Overfitting: Large model, more epochs
    {'embed_size': 256, 'hidden_size': 512, 'num_layers': 3, 'learning_rate': 0.001},
    # Best fit: Medium model, medium epochs
    {'embed_size': 128, 'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.002}
]

# Run experiments (Assuming you have val_dataloader ready)
# results = run_experiments(len(vocab), dataloader, val_dataloader, criterion, device, experiment_configs, num_epochs=10)

from torch.utils.data import random_split, DataLoader

train_ratio = 0.8
train_size = int(train_ratio * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

batch_size = 32

rain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

print(f"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")

train_ratio = 0.8
train_size = int(train_ratio * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

batch_size = 32
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

experiment_configs = [
    {'embed_size': 64, 'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.01},
    {'embed_size': 256, 'hidden_size': 512, 'num_layers': 3, 'learning_rate': 0.001},
    {'embed_size': 128, 'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.002}
]

results = run_experiments(len(vocab), train_dataloader, val_dataloader, criterion, device, experiment_configs, num_epochs=10)

import numpy as np

def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience=3):
    model.train()
    best_val_loss = np.inf
    epochs_no_improve = 0
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        total_train_loss = 0
        # Training loop
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs, _ = model(inputs)
            loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # Validation loss computation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs, _ = model(inputs)
                val_loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))
                total_val_loss += val_loss.item()
        avg_val_loss = total_val_loss / len(val_loader)

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            # Save best model weights
            torch.save(model.state_dict(), 'best_model.pt')
        else:
            epochs_no_improve += 1
            if epochs_no_improve == patience:
                print("Early stopping triggered")
                break
        model.train()
    return train_losses, val_losses

import matplotlib.pyplot as plt

# First, train a model using the `train_with_early_stopping` function
# We'll use one of the configurations from the previous experiments as an example
# For instance, let's use the 'best fit' configuration
config = experiment_configs[2]

# Re-initialize the model, optimizer, and criterion for this training run
model_es = LSTMLanguageModel(
    vocab_size=len(vocab),
    embed_size=config['embed_size'],
    hidden_size=config['hidden_size'],
    num_layers=config['num_layers']
).to(device)
optimizer_es = torch.optim.Adam(model_es.parameters(), lr=config['learning_rate'])

# Call the modified train_with_early_stopping function to get losses
train_losses, val_losses = train_with_early_stopping(
    model_es, train_dataloader, val_dataloader, criterion, optimizer_es, device, num_epochs=20, patience=5
)

# Now plot the collected losses
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss vs Epoch with Early Stopping')
plt.grid(True)
plt.show()

idx_to_token = {idx: token for token, idx in vocab.items()}

def generate_text_adv(model, vocab, idx_to_token, start_tokens, generate_len=50, temperature=0.8):
    model.eval()
    device = next(model.parameters()).device
    input_seq = torch.tensor([vocab[token] for token in start_tokens], dtype=torch.long).unsqueeze(0).to(device)
    generated = start_tokens[:]
    hidden = None

    for _ in range(generate_len):
        with torch.no_grad():
            output, hidden = model(input_seq, hidden)
        logits = output[:, -1, :] / temperature
        probs = torch.softmax(logits, dim=-1)
        next_token_idx = torch.multinomial(probs, num_samples=1).item()
        next_token = idx_to_token[next_token_idx]
        generated.append(next_token)
        input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)

    return ' '.join(generated)

# Example usage
print(generate_text_adv(model, vocab, idx_to_token, ['Elizabeth', 'was']))

import matplotlib.pyplot as plt

def plot_loss(train_losses, val_losses):
    epochs = range(1, len(train_losses) + 1)
    plt.figure(figsize=(8,5))
    plt.plot(epochs, train_losses, 'b-', label='Training Loss')
    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss over Epochs')
    plt.show()

plot_loss(train_losses, val_losses)

